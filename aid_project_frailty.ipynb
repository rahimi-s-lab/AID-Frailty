{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: Feature matrix\n",
    "# y: Target vector\n",
    "\n",
    "cat= X.select_dtypes(include='object').columns\n",
    "num= X.select_dtypes(include=['float','int']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "encoder=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "fill_nan=KNNImputer(n_neighbors=10)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "cat_transformer = Pipeline(\n",
    "    steps=[('encode', encoder), (\"fillna\", fill_nan)]\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([('impute', fill_nan), ('scale', scaler)]), num),\n",
    "        (\"cat\", cat_transformer, cat),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statistics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, average_precision_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import FactorAnalysis, KernelPCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "svm_classifier = SVC()\n",
    "\n",
    "\n",
    "\n",
    "# Feature selection methods\n",
    "feature_selection_methods = {\n",
    "    'Correlation-based Feature Selection': GenericUnivariateSelect(score_func=f_classif, mode='k_best', param=5),  # Adjust k as per requirement\n",
    "    'Mutual information': GenericUnivariateSelect(score_func=mutual_info_classif, mode='k_best', param=5),  # Adjust k as per requirement\n",
    "    'SelectKBest': SelectKBest(score_func=f_classif, k=5),  # Adjust k as per requirement\n",
    "    'Sequential Forward Selection': SequentialFeatureSelector(estimator=svm_classifier, n_features_to_select=5, direction='forward', cv=LeaveOneOut())\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Dimensionality reduction methods\n",
    "reductioner = {\n",
    "    'PCA': PCA(n_components=0.9),\n",
    "    'LDA': LinearDiscriminantAnalysis(n_components=1),\n",
    "    'FA': FactorAnalysis(n_components=2),\n",
    "    'kPCA': KernelPCA(n_components=1, kernel='rbf')\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Base classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(max_depth=7, min_samples_split=5, min_samples_leaf=9),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=2),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(C=1.0),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': XGBClassifier(reg_alpha=0.0, reg_lambda=1.0),\n",
    "    'LightGBM': LGBMClassifier(reg_alpha=0.0, reg_lambda=1.0),\n",
    "    'SVM': SVC(C=1.0),\n",
    "    'Gaussian Process': GaussianProcessClassifier()\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Ensemble classifiers\n",
    "ensemble_classifiers = {\n",
    "    'Voting Classifier': VotingClassifier(estimators=list(classifiers.items()), voting='hard'),\n",
    "    'Bagging Classifier': BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=392, random_state=0),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=392),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=392)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "file_path = 'log.txt'           \n",
    "with open(file_path, 'w') as file:\n",
    "\n",
    "\n",
    "    for method_name, method in feature_selection_methods.items():\n",
    "        for reduction_name, reduction in reductioner.items():\n",
    "            for clf_name, clf in classifiers.items():\n",
    "                y_true_all              = []\n",
    "                y_pred_all              = []\n",
    "                accuracies              = []\n",
    "                f1_scores               = []\n",
    "                precision               = []\n",
    "                recall                  = []\n",
    "                shap_values_list        = []\n",
    "\n",
    "                for _ in range(10):\n",
    "                    print(f\"Evaluating feature selection: {method_name} - feature reduction: {reduction_name} - classifier: {clf_name}\")\n",
    "                    file.write(f\"{method_name} - {reduction_name} - {clf_name} - {_}\\n\")\n",
    "\n",
    "                    kf = KFold(n_splits=4)\n",
    "                                    \n",
    "                    for train_index, test_index in kf.split(X):\n",
    "                        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                        \n",
    "                        \n",
    "                        pipeline = Pipeline([\n",
    "                            ('preprocessor', preprocessor),  # Preprocessing steps defined in your preprocessor\n",
    "                            ('feature_selection', method),  \n",
    "                            ('reduction', reduction),  # for dimensionality reduction\n",
    "                            ('classifier', clf)  # Classifier\n",
    "                        ])\n",
    "                        \n",
    "\n",
    "                        pipeline.fit(X_train, y_train)\n",
    "                        y_pred = pipeline.predict(X_test)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        #SHAP\n",
    "                        explainer = shap.Explainer(pipeline.predict, X_train)\n",
    "                        shap_values = explainer(X_test)\n",
    "                        shap_values_list.append(shap_values.values)\n",
    "                    \n",
    "                        \n",
    "                        \n",
    "                        #LIME\n",
    "                        explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "                            training_data=X.values,\n",
    "                            feature_names=X.columns,\n",
    "                            class_names=['No Frail', 'Frail'],\n",
    "                            discretize_continuous=True\n",
    "                        )\n",
    "                        i = 0\n",
    "                        instance = X.iloc[i].values\n",
    "                        exp = explainer.explain_instance(instance, pipeline.predict_proba, num_features=7)\n",
    "                        exp.show_in_notebook(show_table=True)\n",
    "                        fig = exp.as_pyplot_figure()\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "                        file.write(f\"{accuracies}\\n\")\n",
    "                        \n",
    "                        f1_scores.append(f1_score(y_test, y_pred))\n",
    "                        file.write(f\"{f1_scores}\\n\")\n",
    "                        \n",
    "                        precision.append(precision_score(y_test, y_pred))\n",
    "                        file.write(f\"{precision}\\n\")\n",
    "                        \n",
    "                        recall.append(recall_score(y_test, y_pred))                 \n",
    "                        file.write(f\"{recall}\\n\")\n",
    "                        \n",
    "                        y_true_all.extend(y_test)\n",
    "                        y_pred_all.extend(y_pred)\n",
    "                        file.write(f\"{y_true_all}\\n\")\n",
    "                        file.write(f\"{y_pred_all}\\n\")\n",
    "                    \n",
    "                            \n",
    "                \n",
    "                mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "                std_accuracy = statistics.stdev(accuracies) if len(accuracies) > 1 else 0\n",
    "                \n",
    "                mean_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "                std_f1_score = statistics.stdev(f1_scores) if len(f1_scores) > 1 else 0\n",
    "                \n",
    "                mean_precision = sum(precision) / len(precision)\n",
    "                std_precision = statistics.stdev(precision) if len(precision) > 1 else 0\n",
    "                \n",
    "                mean_recall = sum(recall) / len(recall)\n",
    "                std_recall = statistics.stdev(recall) if len(recall) > 1 else 0\n",
    "                \n",
    "                cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "                cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #SHAP\n",
    "                shap_values_list = np.array(shap_values_list)\n",
    "                aggregated_shap_values = np.mean(shap_values_list, axis=0)\n",
    "                \n",
    "                aggregated_shap_values_object = shap.Explanation(\n",
    "                    values=aggregated_shap_values,\n",
    "                    base_values=shap_values.base_values,  \n",
    "                    data=shap_values.data,                \n",
    "                    feature_names=shap_values.feature_names\n",
    "                )  \n",
    "                plt.figure(figsize=(12, 8))\n",
    "                shap.plots.waterfall(shap_values[0], max_display=14, show=False)\n",
    "                plt.savefig(f\"./waterfall_plot_{method_name}_{reduction_name}_{clf_name}.png\", bbox_inches='tight', dpi=1080)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"Mean Accuracy:\", mean_accuracy)\n",
    "                print(\"STD Accuracy:\", std_accuracy)\n",
    "                file.write(f\"{mean_accuracy}\\n\")\n",
    "                file.write(f\"{std_accuracy}\\n\")\n",
    "                \n",
    "                print(\"Mean F1-score:\", mean_f1_score)\n",
    "                print(\"STD F1-score:\", std_f1_score)\n",
    "                file.write(f\"{mean_f1_score}\\n\")\n",
    "                file.write(f\"{std_f1_score}\\n\")\n",
    "                \n",
    "                print(\"Mean Precision:\", mean_precision)\n",
    "                print(\"STD Precision:\", std_precision)\n",
    "                file.write(f\"{mean_precision}\\n\")\n",
    "                file.write(f\"{std_precision}\\n\")\n",
    "                \n",
    "                print(\"Mean Recall:\", mean_recall)\n",
    "                print(\"STD Recall:\", std_recall)\n",
    "                file.write(f\"{mean_recall}\\n\")\n",
    "                file.write(f\"{std_recall}\\n\")\n",
    "                            \n",
    "                \n",
    "                \n",
    "                results.append({\n",
    "                    'Feature Selection Method': method_name,\n",
    "                    'Dimensionality Reduction Method': reduction_name,\n",
    "                    'Classification Method': clf_name,\n",
    "                    'Mean Accuracy': mean_accuracy,\n",
    "                    'STD Accuracy': std_accuracy,\n",
    "                    'Mean F1-score': mean_f1_score,\n",
    "                    'STD F1-score': std_f1_score,\n",
    "                    'Mean Precision': mean_precision,\n",
    "                    'STD Precision': std_precision,\n",
    "                    'Mean Recall': mean_recall,\n",
    "                    'STD Recall': std_recall,\n",
    "                    'Confusion Matrix': cm\n",
    "                })\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(cm, annot=True, cmap='Blues', fmt='.2f', xticklabels=['Non-Frail', 'Frail'], yticklabels=['Non-Frail', 'Frail'])\n",
    "                plt.title(f\"Confusion Matrix - {method_name} - {reduction_name} - {clf_name}\")\n",
    "                plt.xlabel('Predicted Label')\n",
    "                plt.ylabel('Actual Label')\n",
    "                plt.show()\n",
    "\n",
    "                \n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "    for method_name, method in feature_selection_methods.items():\n",
    "        for reduction_name, reduction in reductioner.items():\n",
    "            for ensemble_name, ensemble_clf in ensemble_classifiers.items():\n",
    "                y_true_all          = []\n",
    "                y_pred_all          = []\n",
    "                accuracies          = []\n",
    "                f1_scores           = []\n",
    "                precision           = []\n",
    "                recall              = []\n",
    "                shap_values_list    = []\n",
    "                \n",
    "                for _ in range(10): \n",
    "                    print(f\"Evaluating feature selection: {method_name} - feature reduction: {reduction_name} - ensemble classifier: {ensemble_name}\")\n",
    "                    file.write(f\"{method_name} - {reduction_name} - {ensemble_name} - {_}\\n\")\n",
    "\n",
    "                    kf = KFold(n_splits=4)\n",
    "                    \n",
    "                    \n",
    "                    for train_index, test_index in kf.split(X):\n",
    "                        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                        \n",
    "                        \n",
    "                        pipeline = Pipeline([\n",
    "                            ('preprocessor', preprocessor), \n",
    "                            ('feature_selection', method),  \n",
    "                            ('reduction', reduction),  \n",
    "                            ('classifier', ensemble_clf)  \n",
    "                        ])\n",
    "\n",
    "\n",
    "                        pipeline.fit(X_train, y_train)\n",
    "                        y_pred = pipeline.predict(X_test)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        #SHAP\n",
    "                        explainer = shap.Explainer(pipeline.predict, X_train)\n",
    "                        shap_values = explainer(X_test)\n",
    "                        shap_values_list.append(shap_values.values)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        #LIME\n",
    "                        explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "                            training_data=X.values,\n",
    "                            feature_names=X.columns,\n",
    "                            class_names=['No Frail', 'Frail'],\n",
    "                            discretize_continuous=True\n",
    "                        )\n",
    "                        i = 0\n",
    "                        instance = X.iloc[i].values\n",
    "                        exp = explainer.explain_instance(instance, pipeline.predict_proba, num_features=7)\n",
    "                        exp.show_in_notebook(show_table=True)\n",
    "                        fig = exp.as_pyplot_figure()\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "                        file.write(f\"{accuracies}\\n\")\n",
    "                       \n",
    "                        f1_scores.append(f1_score(y_test, y_pred))\n",
    "                        file.write(f\"{f1_scores}\\n\")\n",
    "                        \n",
    "                        precision.append(precision_score(y_test, y_pred))\n",
    "                        file.write(f\"{precision}\\n\")\n",
    "                        \n",
    "                        recall.append(recall_score(y_test, y_pred))\n",
    "                        file.write(f\"{recall}\\n\")\n",
    "                        \n",
    "                       \n",
    "                        y_true_all.extend(y_test)\n",
    "                        y_pred_all.extend(y_pred)\n",
    "                        file.write(f\"{y_true_all}\\n\")\n",
    "                        file.write(f\"{y_pred_all}\\n\")\n",
    "                    \n",
    "                mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "                std_accuracy = statistics.stdev(accuracies) if len(accuracies) > 1 else 0\n",
    "                \n",
    "                mean_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "                std_f1_score = statistics.stdev(f1_scores) if len(f1_scores) > 1 else 0\n",
    "                \n",
    "                mean_precision = sum(precision) / len(precision)\n",
    "                std_precision = statistics.stdev(precision) if len(precision) > 1 else 0\n",
    "                \n",
    "                mean_recall = sum(recall) / len(recall)\n",
    "                std_recall = statistics.stdev(recall) if len(recall) > 1 else 0\n",
    "                \n",
    "                cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "                cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                \n",
    "                \n",
    "                \n",
    "                #SHAP\n",
    "                shap_values_list = np.array(shap_values_list)\n",
    "                aggregated_shap_values = np.mean(shap_values_list, axis=0)\n",
    "                \n",
    "                aggregated_shap_values_object = shap.Explanation(\n",
    "                    values=aggregated_shap_values,\n",
    "                    base_values=shap_values.base_values,  \n",
    "                    data=shap_values.data,                \n",
    "                    feature_names=shap_values.feature_names\n",
    "                )  \n",
    "                plt.figure(figsize=(12, 8))\n",
    "                shap.plots.waterfall(shap_values[0], max_display=14, show=False)\n",
    "                plt.savefig(f\"./waterfall_plot_{method_name}_{reduction_name}_{clf_name}.png\", bbox_inches='tight', dpi=1080)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"Mean Accuracy:\", mean_accuracy)\n",
    "                print(\"STD Accuracy:\", std_accuracy)\n",
    "                file.write(f\"{mean_accuracy}\\n\")\n",
    "                file.write(f\"{std_accuracy}\\n\")\n",
    "                \n",
    "                #print(\"Mean AUC:\", mean_auc)\n",
    "                print(\"Mean F1-score:\", mean_f1_score)\n",
    "                print(\"STD F1-score:\", std_f1_score)\n",
    "                file.write(f\"{mean_f1_score}\\n\")\n",
    "                file.write(f\"{std_f1_score}\\n\")\n",
    "                \n",
    "                print(\"Mean Precision:\", mean_precision)\n",
    "                print(\"STD Precision:\", std_precision)\n",
    "                file.write(f\"{mean_precision}\\n\")\n",
    "                file.write(f\"{std_precision}\\n\")\n",
    "                \n",
    "                print(\"Mean Recall:\", mean_recall)\n",
    "                print(\"STD Recall:\", std_recall)\n",
    "                file.write(f\"{mean_recall}\\n\")\n",
    "                file.write(f\"{std_recall}\\n\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                results.append({\n",
    "                    'Feature Selection Method': method_name,\n",
    "                    'Dimensionality Reduction Method': reduction_name,\n",
    "                    'Classification Method': ensemble_name,\n",
    "                    'Mean Accuracy': mean_accuracy,\n",
    "                    'STD Accuracy': std_accuracy,\n",
    "                    'Mean F1-score': mean_f1_score,\n",
    "                    'STD F1-score': std_f1_score,\n",
    "                    'Mean Precision': mean_precision,\n",
    "                    'STD Precision': std_precision,\n",
    "                    'Mean Recall': mean_recall,\n",
    "                    'STD Recall': std_recall,\n",
    "                    'Confusion Matrix': cm\n",
    "                })\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(cm, annot=True, cmap='Blues', fmt='.2f', xticklabels=['Non-Frail', 'Frail'], yticklabels=['Non-Frail', 'Frail'])\n",
    "                plt.title(f\"Confusion Matrix - {method_name} - {reduction_name} - {ensemble_name}\")\n",
    "                plt.xlabel('Predicted Label')\n",
    "                plt.ylabel('Actual Label')\n",
    "                plt.show()\n",
    "                    \n",
    "                   \n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
